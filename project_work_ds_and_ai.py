# -*- coding: utf-8 -*-
"""Project Work DS and AI

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18GiCRkw94_jsmUys-1xO-fkGAJdt9OXi
"""

# Run in a code cell
!pip install -q transformers[torch] datasets rouge-score sentence_transformers sumy nltk gradio kaggle

from google.colab import drive
drive.mount('/content/drive')
# Recommended save path example: /content/drive/MyDrive/review_summarizer

from google.colab import files
uploaded = files.upload()  # upload kaggle.json here if you plan to download from Kaggle

# move to ~/.kaggle
import os
if 'kaggle.json' in uploaded:
    os.makedirs('/root/.kaggle', exist_ok=True)
    open('/root/.kaggle/kaggle.json','wb').write(uploaded['kaggle.json'])
    os.chmod('/root/.kaggle/kaggle.json', 0o600)
    print("kaggle.json saved.")
else:
    print("No kaggle.json uploaded. If you need Kaggle download, upload kaggle.json now.")

!ls -l /root/.kaggle

DATASET_SLUG = "snap/amazon-fine-food-reviews"   # change if you want another dataset

import os
os.makedirs('/content/data', exist_ok=True)
!kaggle datasets download -d {DATASET_SLUG} -p /content/data --unzip
!ls -lh /content/data

import pandas as pd

# Load CSV file
df = pd.read_csv("/content/data/Reviews.csv")

# Check first few rows
df.head()

# Keep only review text and summary
df = df[['Text', 'Summary']].dropna()

# Rename columns for easier access
df.columns = ['review', 'summary']

df.head()

# Remove duplicates
df.drop_duplicates(inplace=True)

# Strip extra spaces and lowercase
df['review'] = df['review'].str.strip().str.lower()
df['summary'] = df['summary'].str.strip().str.lower()

!pip install transformers sentencepiece

from transformers import T5Tokenizer, T5ForConditionalGeneration

# Load tokenizer & model
tokenizer = T5Tokenizer.from_pretrained("t5-small")
model = T5ForConditionalGeneration.from_pretrained("t5-small")

import os, re, math, json
import pandas as pd, numpy as np
import nltk
nltk.download('punkt')
from nltk.tokenize import sent_tokenize
print("Imports done. NLTK punkt ready.")

DATA_DIR = "/content/data"
files = []
if os.path.exists(DATA_DIR):
    files = os.listdir(DATA_DIR)
print("Files in /content/data:", files)

# Try to find a CSV-like file automatically
candidates = [f for f in files if f.lower().endswith(('.csv','.json','.parquet'))]
df = None
if candidates:
    # load the first candidate (adjust if you know filename)
    path = os.path.join(DATA_DIR, candidates[0])
    print("Attempting to load:", path)
    if path.lower().endswith('.csv'):
        df = pd.read_csv(path, low_memory=False)
    elif path.lower().endswith('.json'):
        df = pd.read_json(path, lines=True)
    elif path.lower().endswith('.parquet'):
        df = pd.read_parquet(path)
else:
    print("No dataset files found â€” creating a small fallback sample.")

# Fallback tiny sample if nothing loaded
if df is None or df.shape[0] == 0:
    sample = {
        "product_id": ["P1","P1","P2","P2"],
        "reviewText": [
            "Great product, arrived fast. Battery life is excellent and the display is bright.",
            "Works well but the build feels cheap. Customer support was helpful though.",
            "Terrible experience, stopped working after a week. Refund took long.",
            "Good value for money. I recommend for budget buyers."
        ],
        "summary": ["Good product","Mixed feelings","Broken after week","Value buy"],
        "overall": [5,3,1,4]
    }
    df = pd.DataFrame(sample)
    print("Using sample dataframe with", df.shape[0], "rows")

print("Loaded dataframe shape:", df.shape)
display(df.head())

# try to detect review & summary columns
possible_review_cols = ['reviewText','review_text','text','review','review_body','content']
possible_summary_cols = ['summary','summary_text','headline','title']

review_col = next((c for c in df.columns if c in possible_review_cols), None)
summary_col = next((c for c in df.columns if c in possible_summary_cols), None)
prod_col = next((c for c in df.columns if c.lower() in ['product_id','asin','productid']), None)

print("Detected columns -> review:", review_col, " summary:", summary_col, " product:", prod_col)

# create clean_review column
def clean_text(text):
    if pd.isna(text): return ""
    text = str(text)
    text = re.sub(r'http\S+','', text)
    text = re.sub(r'\s+',' ', text)
    return text.strip()

# Use the detected review_col if found, otherwise fallback to 'Text'
review_column_to_use = review_col if review_col else 'Text'
df['clean_review'] = df[review_column_to_use].astype(str).map(clean_text)
df['n_words'] = df['clean_review'].map(lambda t: len(t.split()))
print("Basic stats")
print(df[['n_words']].describe())

# show hist and some example reviews
try:
    import matplotlib.pyplot as plt
    plt.hist(df['n_words'], bins=20)
    plt.title('Review length (words)')
    plt.xlabel('words')
    plt.show()
except Exception as e:
    print("Plotting skipped:", e)

display(df[['clean_review','n_words']].head(10))

from nltk.tokenize import sent_tokenize
import nltk

def split_sentences(text):
    nltk.download('punkt_tab', quiet=True) # Download the missing resource
    return sent_tokenize(text)

def chunk_text_by_words(text, max_words=400):
    words = text.split()
    chunks = []
    for i in range(0, len(words), max_words):
        chunks.append(" ".join(words[i:i+max_words]))
    return chunks

# quick test
example = df['clean_review'].iloc[0]
print("Sentences:", split_sentences(example))
print("Chunks:", chunk_text_by_words(example, max_words=20))

from sumy.parsers.plaintext import PlaintextParser
from sumy.nlp.tokenizers import Tokenizer
from sumy.summarizers.text_rank import TextRankSummarizer

def extractive_summary(text, sent_count=3):
    parser = PlaintextParser.from_string(text, Tokenizer("english"))
    summarizer = TextRankSummarizer()
    summary_sentences = summarizer(parser.document, sent_count)
    return " ".join([str(s) for s in summary_sentences])

# test extractive baseline on a sample review or concatenation
sample_text = " ".join(df['clean_review'].iloc[:2].tolist())
print("Extractive summary (2 sentences):\n", extractive_summary(sample_text, sent_count=2))

from transformers import pipeline
import torch

# choose model (change if needed)
MODEL = "facebook/bart-large-cnn"   # good default for summarization
device = 0 if torch.cuda.is_available() else -1
print("Using device:", device, " torch.cuda.is_available():", torch.cuda.is_available())

summarizer = pipeline("summarization", model=MODEL, device=device)
print("Summarizer pipeline loaded.")

def abstractive_summarize_long(text, max_chunk_words=400, max_summary_length=120, min_summary_length=30):
    text = clean_text(text)
    if len(text.split()) <= max_chunk_words:
        out = summarizer(text, max_length=max_summary_length, min_length=min_summary_length, do_sample=False)
        return out[0]['summary_text']
    # chunk, summarize each, then compress
    chunks = chunk_text_by_words(text, max_chunk_words)
    partial_summaries = []
    for c in chunks:
        res = summarizer(c, max_length=max_summary_length, min_length=min_summary_length, do_sample=False)
        partial_summaries.append(res[0]['summary_text'])
    joined = " ".join(partial_summaries)
    final = summarizer(joined, max_length=max_summary_length, min_length=min_summary_length, do_sample=False)
    return final[0]['summary_text']

# quick test on a concatenation of a few reviews
text_for_test = " ".join(df['clean_review'].iloc[:4].tolist())
print("Abstractive summary:\n", abstractive_summarize_long(text_for_test, max_chunk_words=200))

!pip install -q sumy

import pandas as pd
import os
import re
import nltk
from nltk.tokenize import sent_tokenize
from sumy.parsers.plaintext import PlaintextParser
from sumy.nlp.tokenizers import Tokenizer
from sumy.summarizers.text_rank import TextRankSummarizer
from transformers import pipeline
import torch

nltk.download('punkt', quiet=False)
nltk.download('punkt_tab', quiet=False)

# Load CSV file
DATA_DIR = "/content/data"
files = []
if os.path.exists(DATA_DIR):
    files = os.listdir(DATA_DIR)

df = None
candidates = [f for f in files if f.lower().endswith(('.csv','.json','.parquet'))]
if candidates:
    path = os.path.join(DATA_DIR, candidates[0])
    if path.lower().endswith('.csv'):
        df = pd.read_csv(path, low_memory=False)
    elif path.lower().endswith('.json'):
        df = pd.read_json(path, lines=True)
    elif path.lower().endswith('.parquet'):
        df = pd.read_parquet(path)

# Fallback tiny sample if nothing loaded
if df is None or df.shape[0] == 0:
    sample = {
        "product_id": ["P1","P1","P2","P2"],
        "reviewText": [
            "Great product, arrived fast. Battery life is excellent and the display is bright.",
            "Works well but the build feels cheap. Customer support was helpful though.",
            "Terrible experience, stopped working after a week. Refund took long.",
            "Good value for money. I recommend for budget buyers."
        ],
        "summary": ["Good product","Mixed feelings","Broken after week","Value buy"],
        "overall": [5,3,1,4]
    }
    df = pd.DataFrame(sample)


# try to detect review & summary columns
possible_review_cols = ['reviewText','review_text','text','review','review_body','content']
possible_summary_cols = ['summary','summary_text','headline','title']

review_col = next((c for c in df.columns if c in possible_review_cols), None)
summary_col = next((c for c in df.columns if c in possible_summary_cols), None)
prod_col = next((c for c in df.columns if c.lower() in ['product_id','asin','productid']), None)

# create clean_review column
def clean_text(text):
    if pd.isna(text): return ""
    text = str(text)
    text = re.sub(r'http\S+','', text)
    text = re.sub(r'\s+',' ', text)
    return text.strip()

# Use the detected review_col if found, otherwise fallback to 'Text'
review_column_to_use = review_col if review_col else 'Text'
df['clean_review'] = df[review_column_to_use].astype(str).map(clean_text)

# Define summarization functions within this cell
def split_sentences(text):
    # nltk.download('punkt_tab', quiet=True) # Download the missing resource - downloaded at the beginning of cell
    return sent_tokenize(text)

def chunk_text_by_words(text, max_words=400):
    words = text.split()
    chunks = []
    for i in range(0, len(words), max_words):
        chunks.append(" ".join(words[i:i+max_words]))
    return chunks

def extractive_summary(text, sent_count=3):
    parser = PlaintextParser.from_string(text, Tokenizer("english"))
    summarizer = TextRankSummarizer()
    summary_sentences = summarizer(parser.document, sent_count)
    return " ".join([str(s) for s in summary_sentences])

# choose model (change if needed)
MODEL = "facebook/bart-large-cnn"   # good default for summarization
device = 0 if torch.cuda.is_available() else -1
print("Using device:", device, " torch.cuda.is_available():", torch.cuda.is_available())

summarizer = pipeline("summarization", model=MODEL, device=device)
print("Summarizer pipeline loaded.")


def abstractive_summarize_long(text, max_chunk_words=400, max_summary_length=120, min_summary_length=30):
    text = clean_text(text)
    if len(text.split()) <= max_chunk_words:
        out = summarizer(text, max_length=max_summary_length, min_length=min_summary_length, do_sample=False)
        return out[0]['summary_text']
    # chunk, summarize each, then compress
    chunks = chunk_text_by_words(text, max_chunk_words)
    partial_summaries = []
    for c in chunks:
        res = summarizer(c, max_length=max_summary_length, min_length=min_summary_length, do_sample=False)
        partial_summaries.append(res[0]['summary_text'])
    joined = " ".join(partial_summaries)
    final = summarizer(joined, max_length=max_summary_length, min_length=min_summary_length, do_sample=False)
    return final[0]['summary_text']


# Example: group reviews by product and summarize (adjust group logic as needed)
if prod_col:
    grouped = df.groupby(prod_col).agg({'clean_review': lambda texts: " ".join(texts.astype(str).tolist())}).reset_index()
else:
    grouped = df.head(50).reset_index().rename(columns={'index':'product_id'})

# For speed while testing, just do first 10 groups
grouped = grouped.head(10).copy()
grouped['extractive'] = grouped['clean_review'].apply(lambda t: extractive_summary(t, sent_count=3))
grouped['abstractive'] = grouped['clean_review'].apply(lambda t: abstractive_summarize_long(t, max_chunk_words=300, max_summary_length=120))

display(grouped[['clean_review','extractive','abstractive']].head())

!pip install -q rouge-score

from rouge_score import rouge_scorer
import numpy as np # Import numpy
import pandas as pd
import os
import re
from transformers import pipeline
import torch

# Load CSV file
DATA_DIR = "/content/data"
files = []
if os.path.exists(DATA_DIR):
    files = os.listdir(DATA_DIR)

df = None
candidates = [f for f in files if f.lower().endswith(('.csv','.json','.parquet'))]
if candidates:
    path = os.path.join(DATA_DIR, candidates[0])
    if path.lower().endswith('.csv'):
        df = pd.read_csv(path, low_memory=False)
    elif path.lower().endswith('.json'):
        df = pd.read_json(path, lines=True)
    elif path.lower().endswith('.parquet'):
        df = pd.read_parquet(path)

# Fallback tiny sample if nothing loaded
if df is None or df.shape[0] == 0:
    sample = {
        "product_id": ["P1","P1","P2","P2"],
        "reviewText": [
            "Great product, arrived fast. Battery life is excellent and the display is bright.",
            "Works well but the build feels cheap. Customer support was helpful though.",
            "Terrible experience, stopped working after a week. Refund took long.",
            "Good value for money. I recommend for budget buyers."
        ],
        "summary": ["Good product","Mixed feelings","Broken after week","Value buy"],
        "overall": [5,3,1,4]
    }
    df = pd.DataFrame(sample)


# try to detect review & summary columns
possible_review_cols = ['reviewText','review_text','text','review','review_body','content']
possible_summary_cols = ['summary','summary_text','headline','title']

review_col = next((c for c in df.columns if c in possible_review_cols), None)
summary_col = next((c for c in df.columns if c in possible_summary_cols), None)
prod_col = next((c for c in df.columns if c.lower() in ['product_id','asin','productid']), None)

# create clean_review column
def clean_text(text):
    if pd.isna(text): return ""
    text = str(text)
    text = re.sub(r'http\S+','', text)
    text = re.sub(r'\s+',' ', text)
    return text.strip()

# Use the detected review_col if found, otherwise fallback to 'Text'
review_column_to_use = review_col if review_col else 'Text'
df['clean_review'] = df[review_column_to_use].astype(str).map(clean_text)

# Define chunking function
def chunk_text_by_words(text, max_words=400):
    words = text.split()
    chunks = []
    for i in range(0, len(words), max_words):
        chunks.append(" ".join(words[i:i+max_words]))
    return chunks

# Define abstractive summarization function and dependencies within this cell
# choose model (change if needed)
MODEL = "facebook/bart-large-cnn"   # good default for summarization
device = 0 if torch.cuda.is_available() else -1
print("Using device:", device, " torch.cuda.is_available():", torch.cuda.is_available())

summarizer = pipeline("summarization", model=MODEL, device=device)
print("Summarizer pipeline loaded.")

def abstractive_summarize_long(text, max_chunk_words=400, max_summary_length=120, min_summary_length=30):
    text = clean_text(text)
    if len(text.split()) <= max_chunk_words:
        out = summarizer(text, max_length=max_summary_length, min_length=min_summary_length, do_sample=False)
        return out[0]['summary_text']
    # chunk, summarize each, then compress
    chunks = chunk_text_by_words(text, max_chunk_words)
    partial_summaries = []
    for c in chunks:
        res = summarizer(c, max_length=max_summary_length, min_length=min_summary_length, do_sample=False)
        partial_summaries.append(res[0]['summary_text'])
    joined = " ".join(partial_summaries)
    final = summarizer(joined, max_length=max_summary_length, min_length=min_summary_length, do_sample=False)
    return final[0]['summary_text']


scorer = rouge_scorer.RougeScorer(['rouge1','rouge2','rougeL'], use_stemmer=True)

def rouge_scores(hyp, ref):
    return scorer.score(ref, hyp)

# If original dataset had human summaries, compute ROUGE on a small sample
if summary_col and summary_col in df.columns:
    sample = df.dropna(subset=[summary_col]).head(50)
    pairs = [(abstractive_summarize_long(r), s) for r,s in zip(sample['clean_review'], sample[summary_col])]
    # compute scores
    agg = {'rouge1':[], 'rouge2':[], 'rougeL':[]}
    for hyp, ref in pairs:
        sc = rouge_scores(hyp, str(ref))
        for m in agg:
            agg[m].append(sc[m].fmeasure)
    for m in agg:
        print(m, "avg f-measure:", np.mean(agg[m]) if agg[m] else None)
else:
    print("No reference summaries detected. Create a small human-labeled set for ROUGE or do manual inspection.")

import gradio as gr

def demo_fn(text, extract_sentences, max_len):
    ext = extractive_summary(text, sent_count=extract_sentences)
    absum = abstractive_summarize_long(text, max_chunk_words=400, max_summary_length=max_len)
    return ext, absum

demo = gr.Interface(
    fn=demo_fn,
    inputs=[gr.Textbox(lines=8, placeholder="Paste concatenated reviews or one long string here..."),
            gr.Slider(1,5, value=3, step=1, label="Extractive sentences"),
            gr.Slider(30,300, value=120, step=10, label="Max abstractive length")],
    outputs=["text","text"],
    title="Customer Review Summarizer",
    description="Extractive (TextRank) + Abstractive (BART)"
)

url_info = demo.launch(share=True)
print(url_info)  # public link appears here

# save grouped results to CSV
out_dir = "/content/results"
os.makedirs(out_dir, exist_ok=True)
grouped[['product_id','extractive','abstractive']].to_csv(os.path.join(out_dir,"summaries.csv"), index=False)
print("Saved summaries to:", os.path.join(out_dir,"summaries.csv"))

# if you mounted Drive, copy results to Drive
if os.path.exists('/content/drive'):
    drive_out = "/content/drive/MyDrive/review_summarizer_results"
    os.makedirs(drive_out, exist_ok=True)
    !cp -r {out_dir} {drive_out}
    print("Copied results to Drive:", drive_out)